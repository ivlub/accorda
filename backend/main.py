from fastapi import FastAPI, HTTPException, File, UploadFile, Response, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import os
import shutil
import logging
from pathlib import Path
from typing import List, Literal, Dict, Optional
from jinja2 import Environment, FileSystemLoader, select_autoescape
import json
import uuid
import difflib

# Import services
from ai_service import generate_text_from_gemini
from text_extractor import extract_text

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# Define upload directory within the container
UPLOAD_DIR = Path("/app/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# Define template directory
TEMPLATE_DIR = Path("/app/templates") # Assuming templates are at /app/templates in container
PROMPT_DIR = Path("/app/prompts") # Define prompt directory

# Combine template and prompt directories for Jinja
LOADER_DIRS = []
if TEMPLATE_DIR.is_dir():
    LOADER_DIRS.append(str(TEMPLATE_DIR))
else:
    logger.warning(f"Template directory not found: {TEMPLATE_DIR}")

if PROMPT_DIR.is_dir():
    LOADER_DIRS.append(str(PROMPT_DIR))
else:
    logger.warning(f"Prompt directory not found: {PROMPT_DIR}")

# Setup Jinja2 environment if at least one directory exists
if LOADER_DIRS:
    jinja_env = Environment(
        loader=FileSystemLoader(LOADER_DIRS), # Load from multiple directories
        autoescape=select_autoescape(['html', 'xml', 'jinja'])
    )
    logger.info(f"Jinja2 environment loaded from {LOADER_DIRS}")
else:
    jinja_env = None
    logger.error("Neither template nor prompt directories found. Jinja environment not loaded.")

# Define contract categories
# TODO: Consider moving this to a config file or database
CONTRACT_CATEGORIES = [
    "Sale and purchase",
    "Service provision",
    "Mandate",
    "Deposit",
    "Contract work",
    "Transport",
    "For use of a tangible good",
    "Money for money: Loan",
    "For use of an intangible good",
    "License agreement",
    "For access",
    "Donation",
    "Commodatum",
    "Association contract",
    "Cooperative contract",
    "Company contract",
    "Distribution/Franchise contract",
    "Guarantee contracts",
    "Gambling contract",
    "Contract in favor of a third party",
    "Contract by person to be named",
    "Contract of assignment of contractual position: Promise to contract",
    "Non-Disclosure Agreement",
    "Intellectual Property Agreement",
]

# Allow CORS for frontend development
origins = [
    "http://localhost",
    "http://localhost:5173",
    "http://127.0.0.1:5173",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Health check / Hello World Endpoint --- 
@app.get("/api/hello")
async def read_root():
    return {"message": "Hello from FastAPI!"}

# --- New AI Endpoint --- 

class AIPrompt(BaseModel):
    prompt: str
    model: str | None = None # Optional: specify a model like 'gemini-pro'

@app.post("/api/generate")
async def generate_ai_text(request: AIPrompt):
    """Receives a prompt and returns text generated by the AI service."""
    if not request.prompt:
        raise HTTPException(status_code=400, detail="Prompt cannot be empty")
    
    # Use the provided model or the default from ai_service
    model_to_use = request.model if request.model else None 
    
    try:
        if model_to_use:
            generated_text = await generate_text_from_gemini(request.prompt, model_name=model_to_use)
        else:
            generated_text = await generate_text_from_gemini(request.prompt)
        
        if generated_text.startswith("Error:"):
            # Handle errors reported by the service itself
            raise HTTPException(status_code=500, detail=generated_text)
            
        return {"prompt": request.prompt, "response": generated_text}
    except Exception as e:
        # Catch potential unexpected errors during the call
        # Log the error e
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {e}")

# --- File Extraction Endpoint --- 

@app.post("/api/extract_text")
async def handle_text_extraction(file: UploadFile = File(...) ):
    """Receives a file (PDF or DOCX), extracts text, and returns it."""
    
    # Basic validation for filename and extension
    filename = file.filename
    if not filename:
        raise HTTPException(status_code=400, detail="No filename provided.")
        
    file_extension = Path(filename).suffix.lower()
    if file_extension not in [".pdf", ".docx"]:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}'. Please upload a .pdf or .docx file.")

    # Define temporary file path
    temp_file_path = UPLOAD_DIR / filename 
    # Warning: This simple naming can cause conflicts if multiple users upload files
    # with the same name concurrently. A better approach would use unique IDs (e.g., uuid).
    
    logger.info(f"Receiving file: {filename}. Saving temporarily to {temp_file_path}")

    try:
        # Save the uploaded file temporarily
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            logger.info(f"Successfully saved temporary file: {temp_file_path}")

        # Extract text using the utility function
        extracted_content = extract_text(str(temp_file_path))

        if extracted_content is None:
            # extract_text logs errors, but we should inform the client
            logger.warning(f"Text extraction returned None for file: {filename}")
            # Check if file still exists before raising; maybe it was invalid
            if temp_file_path.exists():
                 raise HTTPException(status_code=500, detail="Failed to extract text from the file. It might be corrupted or empty.")
            else:
                 # This case shouldn't happen if saving succeeded, but good to check
                 raise HTTPException(status_code=500, detail="Internal error during file processing.")

        logger.info(f"Successfully extracted text from {filename}")
        return {"filename": filename, "text": extracted_content}

    except HTTPException as http_exc: 
        # Re-raise HTTPExceptions directly (like the 415 Unsupported Media Type)
        raise http_exc
    except Exception as e:
        logger.error(f"Error processing file {filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred while processing the file: {e}")
    finally:
        # Ensure the temporary file is deleted
        if temp_file_path.exists():
            try:
                temp_file_path.unlink()
                logger.info(f"Successfully deleted temporary file: {temp_file_path}")
            except Exception as del_e:
                logger.error(f"Failed to delete temporary file {temp_file_path}: {del_e}")
        # Close the uploaded file handle
        await file.close()

# --- Contract Comparison Endpoint ---

@app.post("/api/compare")
async def compare_contracts(file1: UploadFile = File(...), file2: UploadFile = File(...)):
    """
    Receives two contract files (PDF/DOCX), extracts text, 
    and returns the raw text content of both files as JSON.
    """
    
    # Validate filenames and types
    for file in [file1, file2]:
        if not file.filename:
            raise HTTPException(status_code=400, detail="One or both files are missing filenames.")
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in [".pdf", ".docx"]:
             raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}' in {file.filename}. Only .pdf and .docx supported.")

    # Use unique IDs for temp files
    unique_id_1 = uuid.uuid4()
    unique_id_2 = uuid.uuid4()
    temp_file_path_1 = UPLOAD_DIR / f"compare_{unique_id_1}_{file1.filename}"
    temp_file_path_2 = UPLOAD_DIR / f"compare_{unique_id_2}_{file2.filename}"
    
    logger.info(f"[Compare] Receiving files: {file1.filename}, {file2.filename}")
    extracted_text_1 = None
    extracted_text_2 = None

    try:
        # 1. Save file 1
        logger.info(f"[Compare] Saving temp file 1: {temp_file_path_1}")
        with temp_file_path_1.open("wb") as buffer:
            shutil.copyfileobj(file1.file, buffer)

        # 2. Save file 2
        logger.info(f"[Compare] Saving temp file 2: {temp_file_path_2}")
        with temp_file_path_2.open("wb") as buffer:
            shutil.copyfileobj(file2.file, buffer)

        # 3. Extract text from file 1
        logger.info(f"[Compare] Extracting text from {file1.filename}")
        extracted_text_1 = extract_text(str(temp_file_path_1))
        if extracted_text_1 is None:
            # Use a more descriptive error or default to empty string if preferred
            logger.error(f"[Compare] Text extraction failed for {file1.filename}. Returning empty text.")
            extracted_text_1 = "" # Or raise HTTPException
            # raise HTTPException(status_code=500, detail=f"Failed to extract text from {file1.filename}.")

        # 4. Extract text from file 2
        logger.info(f"[Compare] Extracting text from {file2.filename}")
        extracted_text_2 = extract_text(str(temp_file_path_2))
        if extracted_text_2 is None:
            logger.error(f"[Compare] Text extraction failed for {file2.filename}. Returning empty text.")
            extracted_text_2 = "" # Or raise HTTPException
            # raise HTTPException(status_code=500, detail=f"Failed to extract text from {file2.filename}.")
            
        logger.info(f"[Compare] Text extracted successfully for {file1.filename} and {file2.filename}")

        # 5. Return extracted texts as JSON
        return {
            "filename1": file1.filename,
            "text1": extracted_text_1,
            "filename2": file2.filename,
            "text2": extracted_text_2
        }

    except HTTPException as http_exc:
        # Re-raise client/server errors related to file handling/extraction
        raise http_exc
    except Exception as e:
        logger.error(f"[Compare] Error during text extraction: {e}", exc_info=True)
        # Ensure text defaults are set even if unexpected error occurs before extraction
        if extracted_text_1 is None: extracted_text_1 = "Error extracting content."
        if extracted_text_2 is None: extracted_text_2 = "Error extracting content."
        # Optionally return error within JSON or raise HTTP exception
        # return {"error": f"An unexpected error occurred: {e}"}
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during comparison setup: {e}")
    finally:
        # 6. Clean up temporary files
        for temp_path in [temp_file_path_1, temp_file_path_2]:
            if temp_path and temp_path.exists():
                try:
                    temp_path.unlink()
                    logger.info(f"[Compare] Deleted temporary file: {temp_path}")
                except Exception as del_e:
                    logger.error(f"[Compare] Failed to delete temporary file {temp_path}: {del_e}")
        # Close file handles
        await file1.close()
        await file2.close()
        
# --- Internal Helper Functions ---

async def _get_contract_category(document_text: str, filename: str) -> str:
    """Internal helper to get contract category using AI."""
    if not jinja_env:
        logger.error(f"[_get_contract_category] Jinja environment not loaded for {filename}")
        raise HTTPException(status_code=500, detail="Internal server error: Template engine not available.")

    # Truncate very long text (reuse existing logic)
    MAX_TEXT_LENGTH = 50000 
    if len(document_text) > MAX_TEXT_LENGTH:
        logger.warning(f"[_get_contract_category] Truncating text for {filename} from {len(document_text)} to {MAX_TEXT_LENGTH} characters.")
        processed_text = document_text[:MAX_TEXT_LENGTH] + "... [TRUNCATED]"
    else:
        processed_text = document_text

    # Prepare the prompt
    try:
        template = jinja_env.get_template("category_prompt.jinja")
        prompt = template.render(document_text=processed_text, categories=CONTRACT_CATEGORIES)
        logger.info(f"[_get_contract_category] Generated category prompt for {filename}")
    except Exception as template_e:
        logger.error(f"[_get_contract_category] Failed to render Jinja template for {filename}: {template_e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error: Failed to prepare category prompt.")

    # Call the AI service
    logger.info(f"[_get_contract_category] Sending request to AI for {filename}")
    ai_response_raw = await generate_text_from_gemini(prompt)
    
    if ai_response_raw.startswith("Error:"):
        logger.error(f"[_get_contract_category] AI service returned an error for {filename}: {ai_response_raw}")
        # Propagate specific error details if possible
        raise HTTPException(status_code=502, detail=f"AI service failed during categorization: {ai_response_raw}")
    
    # Process AI response
    suggested_category = ai_response_raw.strip()
    logger.info(f"[_get_contract_category] Received category suggestion '{suggested_category}' for {filename}")

    # Validate category
    if suggested_category not in CONTRACT_CATEGORIES:
        logger.warning(f"[_get_contract_category] AI suggested category '{suggested_category}' which is not in the predefined list for {filename}. Using 'Other'.")
        final_category = "Other" # Or potentially raise an error/return None?
    else:
        final_category = suggested_category

    logger.info(f"[_get_contract_category] Determined category for {filename}: {final_category}")
    return final_category

# --- API Endpoints ---

@app.post("/api/analyze/categorize")
async def categorize_contract(file: UploadFile = File(...)):
    """Receives a contract file, extracts text, asks AI to categorize it, and returns the category."""
    # Keep initial file validation and saving logic
    filename = file.filename
    if not filename:
        raise HTTPException(status_code=400, detail="No filename provided.")
    
    file_extension = Path(filename).suffix.lower()
    if file_extension not in [".pdf", ".docx"]:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}'. Only .pdf and .docx supported for categorization.")

    temp_file_path = UPLOAD_DIR / f"categorize_{filename}" 
    logger.info(f"[Categorize Endpoint] Receiving file: {filename}. Saving temporarily to {temp_file_path}")

    extracted_content = None
    category = None
    try:
        # 1. Save file temporarily
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            logger.info(f"[Categorize Endpoint] Successfully saved temporary file: {temp_file_path}")

        # 2. Extract text
        extracted_content = extract_text(str(temp_file_path))
        if not extracted_content:
            logger.error(f"[Categorize Endpoint] Text extraction failed or yielded empty content for: {filename}")
            raise HTTPException(status_code=500, detail="Failed to extract text from the file.")
        logger.info(f"[Categorize Endpoint] Successfully extracted text from {filename}. Length: {len(extracted_content)}")
        
        # 3. Call the internal helper function to get the category
        category = await _get_contract_category(extracted_content, filename)
        
        return {"filename": filename, "category": category}

    # Catch potential exceptions from file handling or the helper function
    except HTTPException as http_exc:
        # Re-raise known HTTP exceptions (including those from _get_contract_category)
        raise http_exc
    except Exception as e:
        logger.error(f"[Categorize Endpoint] Error processing file {filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during categorization: {e}")
    finally:
        # 4. Clean up temporary file
        if temp_file_path and temp_file_path.exists():
            try:
                temp_file_path.unlink()
                logger.info(f"[Categorize Endpoint] Successfully deleted temporary file: {temp_file_path}")
            except Exception as del_e:
                logger.error(f"[Categorize Endpoint] Failed to delete temporary file {temp_file_path}: {del_e}")
        # Close the uploaded file handle
        if file:
            await file.close()

# --- New Summary Endpoint --- 

@app.post("/api/analyze/summary")
async def summarize_contract(file: UploadFile = File(...)):
    """Receives a contract file, extracts text, asks AI to summarize it, and returns the summary."""
    
    if not jinja_env:
         raise HTTPException(status_code=500, detail="Template engine not initialized. Check server logs.")

    filename = file.filename
    if not filename:
        raise HTTPException(status_code=400, detail="No filename provided.")
        
    # Check supported file types
    file_extension = Path(filename).suffix.lower()
    if file_extension not in [".pdf", ".docx"]:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}'. Only .pdf and .docx supported for summarization.")

    temp_file_path = UPLOAD_DIR / f"summary_{filename}" # Use prefix to avoid potential clashes
    logger.info(f"[Summary] Receiving file: {filename}. Saving temporarily to {temp_file_path}")

    extracted_content = None
    try:
        # 1. Save file temporarily
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            logger.info(f"[Summary] Successfully saved temporary file: {temp_file_path}")

        # 2. Extract text
        extracted_content = extract_text(str(temp_file_path))
        if not extracted_content:
            logger.error(f"[Summary] Text extraction failed or yielded empty content for: {filename}")
            raise HTTPException(status_code=500, detail="Failed to extract text from the file. It might be corrupted, empty, or password-protected.")
        logger.info(f"[Summary] Successfully extracted text from {filename}. Length: {len(extracted_content)}")
        
        # Truncate very long text (consider a larger limit for summaries?)
        MAX_TEXT_LENGTH = 100000 # Example limit, adjust based on model and needs. TODO: evaluate what this should be 
        if len(extracted_content) > MAX_TEXT_LENGTH:
             logger.warning(f"[Summary] Truncating text for {filename} from {len(extracted_content)} to {MAX_TEXT_LENGTH} characters.")
             extracted_content = extracted_content[:MAX_TEXT_LENGTH] + "... [TRUNCATED]"

        # 3. Prepare the prompt using Jinja template
        try:
            template = jinja_env.get_template("summary_prompt.jinja")
            prompt = template.render(document_text=extracted_content)
            logger.info(f"[Summary] Generated prompt for {filename}")
        except Exception as template_e:
            logger.error(f"[Summary] Failed to render Jinja template: {template_e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Internal server error: Failed to prepare analysis prompt.")

        # 4. Call the AI service
        logger.info(f"[Summary] Sending request to AI for {filename}")
        ai_response_raw = await generate_text_from_gemini(prompt) # Consider using a specific model if needed
        
        if ai_response_raw.startswith("Error:"):
            logger.error(f"[Summary] AI service returned an error for {filename}: {ai_response_raw}")
            raise HTTPException(status_code=502, detail=f"AI service failed: {ai_response_raw}")
        
        # 5. Process AI response (basic cleanup)
        summary_text = ai_response_raw.strip()
        logger.info(f"[Summary] Received summary for {filename}. Length: {len(summary_text)}")

        return {"filename": filename, "summary": summary_text}

    except HTTPException as http_exc:
        # Re-raise known HTTP exceptions
        raise http_exc
    except Exception as e:
        logger.error(f"[Summary] Error processing file {filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during summarization: {e}")
    finally:
        # 6. Clean up temporary file
        if temp_file_path and temp_file_path.exists():
            try:
                temp_file_path.unlink()
                logger.info(f"[Summary] Successfully deleted temporary file: {temp_file_path}")
            except Exception as del_e:
                logger.error(f"[Summary] Failed to delete temporary file {temp_file_path}: {del_e}")
        # Close the uploaded file handle
        if file:
            await file.close()

# --- Helper function to load JSON ---
def load_json_file(file_path: Path):
    if not file_path.is_file():
        logger.error(f"JSON file not found: {file_path}")
        return None
    try:
        with file_path.open("r") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading JSON file {file_path}: {e}", exc_info=True)
        return None

# --- Load Schemas at Startup ---
REQUIREMENTS_SCHEMA_DATA = load_json_file(PROMPT_DIR / "requirements_schema.json")
REQUIREMENTS_OUTPUT_EXAMPLE_DATA = load_json_file(PROMPT_DIR / "requirements_output_example.json")
CATEGORY_REQUIREMENTS_SCHEMAS = load_json_file(PROMPT_DIR / "category_requirements_schemas.json")

# --- Pydantic Models ---

class DiffExplanationResponse(BaseModel):
    filename1: str
    filename2: str
    explanation: str

# General Requirements Models
class RequirementCheckResult(BaseModel):
    met: bool | Literal['maybe']
    explanation: str

class CompletenessCheck(BaseModel):
    date_of_contract_formation: RequirementCheckResult
    parties_involved: RequirementCheckResult
    object_of_contract: RequirementCheckResult
    obligations_of_parties: RequirementCheckResult
    date_of_conclusion: RequirementCheckResult | None = None

class PrecisionCheck(BaseModel):
    terms_clearly_defined: RequirementCheckResult

class FirmnessCheck(BaseModel):
    reciprocal_obligations_established: RequirementCheckResult

class FormalAdequacyCheck(BaseModel):
    formal_requirements_met: RequirementCheckResult

class ContractRequirementsResponse(BaseModel):
    Completeness: CompletenessCheck
    Precision: PrecisionCheck
    Firmness: FirmnessCheck
    FormalAdequacy: FormalAdequacyCheck

# Category-Specific Requirements Models (Updated)
class CategoryExtractionResult(BaseModel): # Renamed for clarity
    status: Literal['extracted', 'missing', 'review_needed']
    value: str | None = None # Extracted text or null if missing/review_needed
    location: str | None = None # E.g., "Section 5, Paragraph 2" or null

class CategoryRequirementsResponse(BaseModel):
    category: str
    analysis: Dict[str, CategoryExtractionResult] | None = None 
    message: str | None = None 

# --- API Endpoints ---

@app.post("/api/analyze/check-requirements", response_model=ContractRequirementsResponse)
async def check_contract_requirements(file: UploadFile = File(...)):
    """
    Receives a contract file, extracts text, asks AI to check GENERAL requirements 
    based on the predefined schema, and returns the structured result.
    (Overrides Formal Adequacy check).
    """
    if not jinja_env:
         raise HTTPException(status_code=500, detail="Template engine not initialized.")
    if REQUIREMENTS_SCHEMA_DATA is None or REQUIREMENTS_OUTPUT_EXAMPLE_DATA is None:
         raise HTTPException(status_code=500, detail="Failed to load general requirement definition files.")

    filename = file.filename
    if not filename:
        raise HTTPException(status_code=400, detail="No filename provided.")
    
    file_extension = Path(filename).suffix.lower()
    if file_extension not in [".pdf", ".docx"]:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}'.")

    temp_file_path = UPLOAD_DIR / f"general_req_{filename}" # Use specific prefix
    logger.info(f"[GeneralReq] Receiving file: {filename}. Saving temporarily to {temp_file_path}")

    extracted_content = None
    try:
        # 1. Save & Extract Text (Same as before)
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        extracted_content = extract_text(str(temp_file_path))
        if not extracted_content:
            raise HTTPException(status_code=500, detail="Failed to extract text.")
        logger.info(f"[GeneralReq] Extracted text from {filename}. Length: {len(extracted_content)}")
        
        # Truncate if needed
        MAX_TEXT_LENGTH = 50000 
        if len(extracted_content) > MAX_TEXT_LENGTH:
             processed_text = extracted_content[:MAX_TEXT_LENGTH] + "... [TRUNCATED]"
        else:
             processed_text = extracted_content

        # 2. Prepare Prompt (Using general template)
        try:
            schema_str = json.dumps(REQUIREMENTS_SCHEMA_DATA, indent=2)
            output_format_str = json.dumps(REQUIREMENTS_OUTPUT_EXAMPLE_DATA, indent=2)
            template = jinja_env.get_template("requirements_prompt.jinja") # General prompt template
            prompt = template.render(
                document_text=processed_text, 
                requirements_schema_str=schema_str,
                output_format_example_str=output_format_str
            )
        except Exception as template_e:
            logger.error(f"[GeneralReq] Failed render template: {template_e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Failed to prepare general analysis prompt.")

        # 3. Call AI
        ai_response_raw = await generate_text_from_gemini(prompt)
        if ai_response_raw.startswith("Error:"):
            raise HTTPException(status_code=502, detail=f"AI service failed (general): {ai_response_raw}")

        # 4. Process and Validate Response (Including 'maybe' fix)
        ai_response_json_str = ai_response_raw.strip()
        if ai_response_json_str.startswith("```json"):
            ai_response_json_str = ai_response_json_str[7:]
        if ai_response_json_str.startswith("```"):
             ai_response_json_str = ai_response_json_str[3:]
        if ai_response_json_str.endswith("```"):
            ai_response_json_str = ai_response_json_str[:-3]
        ai_response_json_str = ai_response_json_str.strip()
        ai_response_json_str = ai_response_json_str.replace(': maybe', ': "maybe"') 

        try:
            ai_response_data = json.loads(ai_response_json_str)
            validated_response = ContractRequirementsResponse(**ai_response_data)
            logger.info(f"[GeneralReq] Parsed/validated general AI response for {filename}")

            # 5. Override Formal Adequacy (Moved back here)
            logger.info(f"[GeneralReq] Overriding FormalAdequacy result for {filename}")
            validated_response.FormalAdequacy.formal_requirements_met.met = "maybe"
            validated_response.FormalAdequacy.formal_requirements_met.explanation = "For formal adequacy, verify the applicable legal requirements in the relevant jurisdiction for the specific type of contract."
            
            return validated_response
        except Exception as e:
            logger.error(f"[GeneralReq] Failed parse/validate general AI response: {e}. Cleaned: {ai_response_json_str}", exc_info=True)
            raise HTTPException(status_code=502, detail=f"General AI response failed validation: {e}")

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"[GeneralReq] Error processing file {filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Unexpected error during general requirements check: {e}")
    finally:
        # Clean up
        if temp_file_path.exists():
            try: temp_file_path.unlink() 
            except Exception as del_e: logger.error(f"Failed delete {temp_file_path}: {del_e}")
        if file: await file.close()

@app.post("/api/analyze/check-category-requirements", response_model=CategoryRequirementsResponse)
async def check_category_requirements(file: UploadFile = File(...)):
    """
    Receives a contract file, determines its category, finds the category-specific 
    schema (focused on EXTRACTION), asks AI to extract information based on THAT schema, 
    and returns the structured result.
    """
    if not jinja_env:
        raise HTTPException(status_code=500, detail="Template engine not initialized.")
    if CATEGORY_REQUIREMENTS_SCHEMAS is None:
        raise HTTPException(status_code=500, detail="Failed to load category requirement definition files.")

    filename = file.filename
    if not filename:
        raise HTTPException(status_code=400, detail="No filename provided.")
    
    file_extension = Path(filename).suffix.lower()
    if file_extension not in [".pdf", ".docx"]:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}'.")

    temp_file_path = UPLOAD_DIR / f"cat_req_{filename}" # Unique prefix
    logger.info(f"[CatReq] Receiving file: {filename}. Saving temporarily to {temp_file_path}")

    extracted_content = None
    category = None
    try:
        # 1. Save & Extract Text 
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        extracted_content = extract_text(str(temp_file_path))
        if not extracted_content:
            raise HTTPException(status_code=500, detail="Failed to extract text.")
        logger.info(f"[CatReq] Extracted text from {filename}. Length: {len(extracted_content)}")

        # 2. Determine Contract Category (using internal helper)
        category = await _get_contract_category(extracted_content, filename)

        # 3. Find Category-Specific Schema
        category_schema = CATEGORY_REQUIREMENTS_SCHEMAS.get(category)
        if not category_schema or "criteria" not in category_schema or "output_format_example" not in category_schema:
            logger.warning(f"[CatReq] No valid schema found for '{category}' for file {filename}")
            # Return the new response model structure
            return CategoryRequirementsResponse(category=category, analysis=None, message=f"Specific analysis not available for category: {category}")

        # 4. Prepare the Prompt (using category template and NEW schema)
        # Truncate text if necessary 
        MAX_TEXT_LENGTH = 50000 
        if len(extracted_content) > MAX_TEXT_LENGTH:
             processed_text = extracted_content[:MAX_TEXT_LENGTH] + "... [TRUNCATED]"
        else:
             processed_text = extracted_content
        
        try:
            # These are now expected to contain the new {status, value, location} structure
            schema_str = json.dumps(category_schema["criteria"], indent=2)
            output_format_str = json.dumps(category_schema["output_format_example"], indent=2)
            template = jinja_env.get_template("category_requirements_prompt.jinja") # Assume this template asks for extraction now
            prompt = template.render(
                document_text=processed_text, 
                contract_category=category,
                requirements_schema_str=schema_str, # Schema defines what to extract
                output_format_example_str=output_format_str # Example shows HOW to extract
            )
            logger.info(f"[CatReq] Generated category extraction prompt for {filename}")
        except Exception as template_e:
            logger.error(f"[CatReq] Failed render category template: {template_e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Failed to prepare category analysis prompt.")

        # 5. Call the AI service
        # Consider using a model better suited for extraction if needed, e.g., Gemini Pro 1.5
        ai_response_raw = await generate_text_from_gemini(prompt)
        if ai_response_raw.startswith("Error:"):
            raise HTTPException(status_code=502, detail=f"AI service failed (category extraction): {ai_response_raw}")

        # 6. Process and validate AI response (for the new structure)
        ai_response_json_str = ai_response_raw.strip()
        # Clean potential markdown code blocks
        if ai_response_json_str.startswith("```json"):
            ai_response_json_str = ai_response_json_str[7:]
        if ai_response_json_str.startswith("```"):
             ai_response_json_str = ai_response_json_str[3:]
        if ai_response_json_str.endswith("```"):
            ai_response_json_str = ai_response_json_str[:-3]
        ai_response_json_str = ai_response_json_str.strip()
        # Removed: ai_response_json_str = ai_response_json_str.replace(': maybe', ': "maybe"') 

        try:
            ai_response_data = json.loads(ai_response_json_str)
            if not isinstance(ai_response_data, dict):
                 raise ValueError("AI response is not a dict.")
            
            # Validate each item against the new CategoryExtractionResult model
            validated_analysis: Dict[str, CategoryExtractionResult] = {}
            for key, value_dict in ai_response_data.items():
                # Ensure the value from AI is a dictionary before attempting validation
                if not isinstance(value_dict, dict):
                    logger.warning(f"[CatReq] Invalid item type for key '{key}' in AI response: {type(value_dict)}. Expected dict.")
                    # Option 1: Skip this invalid item
                    # continue 
                    # Option 2: Create a default 'review_needed' entry
                    validated_analysis[key] = CategoryExtractionResult(
                        status='review_needed', 
                        value=f"Invalid format received from AI: {value_dict}", 
                        location=None
                    )
                    continue
                    # Option 3: Fail the whole request (rely on outer try/except)
                    # raise ValueError(f"Invalid item type for key '{key}': {type(value_dict)}")
                
                # Attempt to validate the dictionary using the Pydantic model
                try:
                    validated_analysis[key] = CategoryExtractionResult(**value_dict) 
                except Exception as item_validation_error:
                    logger.warning(f"[CatReq] Validation failed for item '{key}': {item_validation_error}. Raw item: {value_dict}")
                    # Similar options as above for handling item-level validation failure
                    validated_analysis[key] = CategoryExtractionResult(
                        status='review_needed', 
                        value=f"Validation failed for AI response item: {value_dict}. Error: {item_validation_error}", 
                        location=None
                    )
            
            logger.info(f"[CatReq] Parsed/validated category extraction AI response for {filename}")
            # Return the fully validated response object
            return CategoryRequirementsResponse(category=category, analysis=validated_analysis)
        except json.JSONDecodeError as json_e:
             logger.error(f"[CatReq] Failed to decode JSON from category AI response: {json_e}. Raw: {ai_response_json_str}", exc_info=True)
             raise HTTPException(status_code=502, detail=f"Category AI response was not valid JSON.")
        except Exception as e:
             logger.error(f"[CatReq] Failed parse/validate category AI response: {e}. Cleaned: {ai_response_json_str}", exc_info=True)
             raise HTTPException(status_code=502, detail=f"Category AI response failed validation: {e}")

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"[CatReq] Error processing file {filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Unexpected error during category requirements check: {e}")
    finally:
        # Clean up
        if temp_file_path.exists():
            try: temp_file_path.unlink() 
            except Exception as del_e: logger.error(f"Failed delete {temp_file_path}: {del_e}")
        if file: await file.close()

# --- Diff Explanation Endpoint ---

@app.post("/api/explain-diff", response_model=DiffExplanationResponse)
async def explain_contract_diff(file1: UploadFile = File(...), file2: UploadFile = File(...)):
    """
    Receives two contract files, generates a diff, asks AI to explain the diff,
    and returns the explanation.
    """
    if not jinja_env:
        raise HTTPException(status_code=500, detail="Template engine not initialized.")
    # 1. Validate filenames and types
    for file in [file1, file2]:
        if not file.filename:
            raise HTTPException(status_code=400, detail="One or both files are missing filenames.")
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in [".pdf", ".docx"]:
             raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}' in {file.filename}.")

    # 2. Setup temporary file paths with unique IDs
    unique_id_1 = uuid.uuid4()
    unique_id_2 = uuid.uuid4()
    temp_file_path_1 = UPLOAD_DIR / f"explain_{unique_id_1}_{file1.filename}"
    temp_file_path_2 = UPLOAD_DIR / f"explain_{unique_id_2}_{file2.filename}"
    logger.info(f"[ExplainDiff] Receiving files: {file1.filename}, {file2.filename}")

    extracted_text_1 = None
    extracted_text_2 = None

    try:
        # 3. Save files temporarily
        logger.info(f"[ExplainDiff] Saving temp file 1: {temp_file_path_1}")
        with temp_file_path_1.open("wb") as buffer:
            shutil.copyfileobj(file1.file, buffer)
        logger.info(f"[ExplainDiff] Saving temp file 2: {temp_file_path_2}")
        with temp_file_path_2.open("wb") as buffer:
            shutil.copyfileobj(file2.file, buffer)

        # 4. Extract text from both files
        logger.info(f"[ExplainDiff] Extracting text from {file1.filename}")
        extracted_text_1 = extract_text(str(temp_file_path_1))
        if extracted_text_1 is None:
            logger.error(f"[ExplainDiff] Text extraction failed for {file1.filename}. Treating as empty.")
            extracted_text_1 = "" # Default to empty string

        logger.info(f"[ExplainDiff] Extracting text from {file2.filename}")
        extracted_text_2 = extract_text(str(temp_file_path_2))
        if extracted_text_2 is None:
            logger.error(f"[ExplainDiff] Text extraction failed for {file2.filename}. Treating as empty.")
            extracted_text_2 = "" # Default to empty string

        # 5. Generate Unified Diff
        logger.info(f"[ExplainDiff] Generating diff between extracted texts.")
        # Split texts into lines for difflib
        lines1 = extracted_text_1.splitlines()
        lines2 = extracted_text_2.splitlines()

        # Generate the diff in unified format
        diff_generator = difflib.unified_diff(
            lines1, lines2,
            fromfile=file1.filename, 
            tofile=file2.filename, 
            lineterm='\n' # Ensure consistent line endings
        )
        unified_diff_str = "\n".join(diff_generator) # Join the generated lines
        logger.info(f"[ExplainDiff] Generated unified diff (length: {len(unified_diff_str)}).")

        # Handle case where there is no difference
        if not unified_diff_str:
            logger.info(f"[ExplainDiff] No differences found between {file1.filename} and {file2.filename}.")
            explanation = "No textual differences found between the two documents."
        else:
            # 6. Prepare AI Prompt
            try:
                template = jinja_env.get_template("diff_explanation_prompt.jinja")
                prompt = template.render(
                    filename1=file1.filename,
                    filename2=file2.filename,
                    unified_diff=unified_diff_str
                )
                logger.info(f"[ExplainDiff] Generated explanation prompt for AI.")
            except Exception as template_e:
                logger.error(f"[ExplainDiff] Failed to render Jinja template: {template_e}", exc_info=True)
                raise HTTPException(status_code=500, detail="Internal server error: Failed to prepare explanation prompt.")

            # 7. Call AI Service
            logger.info(f"[ExplainDiff] Sending request to AI for explanation.")
            ai_response_raw = await generate_text_from_gemini(prompt)

            if ai_response_raw.startswith("Error:"):
                logger.error(f"[ExplainDiff] AI service returned an error: {ai_response_raw}")
                raise HTTPException(status_code=502, detail=f"AI service failed during explanation: {ai_response_raw}")
            
            explanation = ai_response_raw.strip()
            logger.info(f"[ExplainDiff] Received explanation from AI.")

        # 8. Return the response
        return DiffExplanationResponse(
            filename1=file1.filename,
            filename2=file2.filename,
            explanation=explanation
        )

    except HTTPException as http_exc:
        # Re-raise known HTTP exceptions
        raise http_exc
    except Exception as e:
        logger.error(f"[ExplainDiff] Error processing files {file1.filename}/{file2.filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during diff explanation: {e}")
    finally:
        # 9. Clean up temporary files
        for temp_path in [temp_file_path_1, temp_file_path_2]:
            if temp_path and temp_path.exists():
                try:
                    temp_path.unlink()
                    logger.info(f"[ExplainDiff] Deleted temporary file: {temp_path}")
                except Exception as del_e:
                    logger.error(f"[ExplainDiff] Failed to delete temporary file {temp_path}: {del_e}")
        # Close file handles
        await file1.close()
        await file2.close()

# --- Pydantic Models for Diff Impact Analysis ---

class DiffImpactChange(BaseModel):
    category: Literal['Beneficial', 'Likely Neutral', 'Prejudicial', 'Further Review Required']
    affected_party: Literal['Disclosing Party', 'Receiving Party', 'Both Parties', 'Neither Party']
    explanation: str
    change_summary: str 

# DiffImpactAnalysisRequest is no longer needed as perspective is passed via query/form

class DiffImpactAnalysisResponse(BaseModel):
    perspective_filename: str
    analysis: List[DiffImpactChange]
    message: Optional[str] = None # For cases with no diff or errors

# --- Main execution ---
if __name__ == "__main__":
    # Determine host based on environment (e.g., Docker container vs local)
    # Docker typically uses 0.0.0.0 to be accessible outside the container
    host = os.getenv("HOST", "127.0.0.1") 
    port = int(os.getenv("PORT", 8000))
    
    # Log the UPLOAD_DIR and TEMPLATE_DIR at startup
    logger.info(f"FastAPI application starting up...")
    logger.info(f"Upload directory set to: {UPLOAD_DIR.resolve()}")
    if jinja_env:
        logger.info(f"Template directory set to: {TEMPLATE_DIR.resolve()}")
    else:
        logger.warning(f"Template directory not found or not configured.")
        
    uvicorn.run(app, host=host, port=port) 

@app.post("/api/analyze-diff-impact", response_model=DiffImpactAnalysisResponse)
async def analyze_diff_impact(
    perspective_filename: str = Form(...), 
    file1: UploadFile = File(...), 
    file2: UploadFile = File(...)
):
    """
    Receives two contract files and a perspective filename (via form data),
    generates a diff, asks AI to analyze the impact of changes from the 
    given perspective, and returns the structured analysis.
    """
    if not jinja_env:
        raise HTTPException(status_code=500, detail="Template engine not initialized.")

    # 1. Validate filenames and types (simplified check)
    # Ensure filenames are not None before accessing them
    file1_name = file1.filename if file1.filename else ""
    file2_name = file2.filename if file2.filename else ""
    files_dict = {file1_name: file1, file2_name: file2}
    
    # Remove empty string key if filenames were missing
    if "" in files_dict:
        del files_dict[""]
        
    if not file1_name or not file2_name:
        raise HTTPException(status_code=400, detail="One or both files are missing filenames.")

    if perspective_filename not in files_dict:
         raise HTTPException(status_code=400, detail=f"Perspective filename '{perspective_filename}' must match one of the uploaded filenames ('{file1_name}' or '{file2_name}').")
         
    for file in [file1, file2]:
        if not file.filename: # Should be caught above, but double-check
            continue
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in [".pdf", ".docx"]:
             raise HTTPException(status_code=415, detail=f"Unsupported file type: '{file_extension}' in {file.filename}. Only .pdf and .docx supported.")

    # 2. Setup temporary file paths
    unique_id_1 = uuid.uuid4()
    unique_id_2 = uuid.uuid4()
    # Use validated filenames
    temp_file_path_1 = UPLOAD_DIR / f"impact_{unique_id_1}_{file1_name}"
    temp_file_path_2 = UPLOAD_DIR / f"impact_{unique_id_2}_{file2_name}"
    logger.info(f"[ImpactAnalysis] Receiving files: {file1_name}, {file2_name}. Perspective: {perspective_filename}")

    extracted_text_1 = None
    extracted_text_2 = None
    
    # Define temp_paths within the try block scope
    temp_paths_to_clean = [] 

    try:
        # 3. Save files temporarily
        logger.info(f"[ImpactAnalysis] Saving temp file 1: {temp_file_path_1}")
        with temp_file_path_1.open("wb") as buffer:
            shutil.copyfileobj(file1.file, buffer)
        temp_paths_to_clean.append(temp_file_path_1) # Add to list for cleanup

        logger.info(f"[ImpactAnalysis] Saving temp file 2: {temp_file_path_2}")
        with temp_file_path_2.open("wb") as buffer:
            shutil.copyfileobj(file2.file, buffer)
        temp_paths_to_clean.append(temp_file_path_2) # Add to list for cleanup

        # 4. Extract text from both files
        logger.info(f"[ImpactAnalysis] Extracting text from {file1_name}")
        extracted_text_1 = extract_text(str(temp_file_path_1))
        if extracted_text_1 is None: extracted_text_1 = "" # Default to empty

        logger.info(f"[ImpactAnalysis] Extracting text from {file2_name}")
        extracted_text_2 = extract_text(str(temp_file_path_2))
        if extracted_text_2 is None: extracted_text_2 = "" # Default to empty

        # 5. Generate Unified Diff
        logger.info(f"[ImpactAnalysis] Generating diff.")
        lines1 = extracted_text_1.splitlines()
        lines2 = extracted_text_2.splitlines()
        from_file_name = file1_name if file1_name else "file1"
        to_file_name = file2_name if file2_name else "file2"
        diff_generator = difflib.unified_diff(lines1, lines2, fromfile=from_file_name, tofile=to_file_name, lineterm='\n')
        unified_diff_str = "\n".join(diff_generator)
        logger.info(f"[ImpactAnalysis] Generated unified diff (length: {len(unified_diff_str)}).")

        if not unified_diff_str.strip():
            logger.info(f"[ImpactAnalysis] No differences found.")
            return DiffImpactAnalysisResponse(
                perspective_filename=perspective_filename, 
                analysis=[],
                message="No textual differences found between the documents."
            )

        # 6. Prepare AI Prompt using Jinja templates
        try:
            # Load category definitions from file (remains the same)
            try:
                categories_template = jinja_env.get_template("diff_impact_categories.jinja")
                categories_definition_rendered = categories_template.render(perspective_filename=perspective_filename)
            except Exception as cat_e:
                # ... error handling ...
                raise HTTPException(status_code=500, detail="Internal server error: Failed to load category definitions.")
            
            # Load and render the output example template, then modify it if needed or define it here
            # For simplicity here, let's define the example string directly
            output_format_example_rendered = """
            Output Format Example (Respond ONLY with the JSON list, no other text):
            [
              {
                "category": "Beneficial",
                "affected_party": "Disclosing Party", 
                "explanation": "Explanation of why this benefits the Disclosing Party.",
                "change_summary": "Removed clause X, granting more rights to Disclosing Party."
              },
              {
                "category": "Prejudicial",
                "affected_party": "Receiving Party", 
                "explanation": "Explanation of why this disadvantages the Receiving Party.",
                "change_summary": "Added new reporting obligation for Receiving Party in section Y."
              },
              {
                 "category": "Likely Neutral",
                 "affected_party": "Both Parties", 
                 "explanation": "Clarification affecting both parties equally.",
                 "change_summary": "Corrected typo in definition Z."
              }
            ]
            """
            
            # Load the main prompt template
            main_template = jinja_env.get_template("diff_impact_prompt.jinja") 
            
            # Render the main prompt, passing the loaded/updated content
            prompt = main_template.render(
                perspective_filename=perspective_filename,
                filename1=from_file_name, 
                filename2=to_file_name,   
                unified_diff=unified_diff_str,
                categories_definition=categories_definition_rendered, 
                output_format_example=output_format_example_rendered # Use updated example
            )
            logger.info(f"[ImpactAnalysis] Generated impact analysis prompt for AI.")
            
        except Exception as template_e:
            logger.error(f"[ImpactAnalysis] Failed render impact template: {template_e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Internal server error: Failed to prepare impact analysis prompt.")

        # 7. Call AI Service
        logger.info(f"[ImpactAnalysis] Sending request to AI for impact analysis.")
        ai_response_raw = await generate_text_from_gemini(prompt) # Consider Gemini Pro 1.5 for complex analysis

        if ai_response_raw.startswith("Error:"):
            logger.error(f"[ImpactAnalysis] AI service error: {ai_response_raw}")
            raise HTTPException(status_code=502, detail=f"AI service failed during impact analysis: {ai_response_raw}")
        
        # 8. Process and Validate AI Response
        ai_response_json_str = ai_response_raw.strip()
        # More robust cleaning for markdown code blocks
        if ai_response_json_str.startswith("```json"):
            ai_response_json_str = ai_response_json_str[7:]
        elif ai_response_json_str.startswith("```"):
             ai_response_json_str = ai_response_json_str[3:]
        if ai_response_json_str.endswith("```"):
            ai_response_json_str = ai_response_json_str[:-3]
        ai_response_json_str = ai_response_json_str.strip()
        
        # Handle empty response after cleaning
        if not ai_response_json_str:
             logger.warning("[ImpactAnalysis] AI returned an empty response after cleaning.")
             raise HTTPException(status_code=502, detail="AI returned an empty or invalid response for impact analysis.")

        try:
            ai_response_data = json.loads(ai_response_json_str)
            if not isinstance(ai_response_data, list):
                 # Attempt to handle if AI wraps list in a key like "analysis"
                 if isinstance(ai_response_data, dict) and len(ai_response_data) == 1:
                     key = list(ai_response_data.keys())[0]
                     if isinstance(ai_response_data[key], list):
                         logger.warning(f"[ImpactAnalysis] AI response was a dict, extracting list from key '{key}'.")
                         ai_response_data = ai_response_data[key]
                     else:
                          raise ValueError(f"AI response is a dict, but value under key '{key}' is not a list.")
                 else:
                     raise ValueError(f"AI response is not a list as expected. Type: {type(ai_response_data)}")
                 
            # Validate each item in the list
            validated_analysis: List[DiffImpactChange] = []
            for item in ai_response_data:
                 if isinstance(item, dict):
                     try:
                         # Ensure category is valid before creating model instance
                         if item.get('category') not in ['Beneficial', 'Likely Neutral', 'Prejudicial', 'Further Review Required']:
                             logger.warning(f"[ImpactAnalysis] Invalid category '{item.get('category')}' in item: {item}. Skipping.")
                             continue # Skip item with invalid category
                         validated_analysis.append(DiffImpactChange(**item))
                     except Exception as item_val_err:
                         logger.warning(f"[ImpactAnalysis] Validation failed for item: {item}. Error: {item_val_err}. Skipping.")
                         # Optionally skip or add a default error item
                 else:
                     logger.warning(f"[ImpactAnalysis] Skipping non-dict item in AI response list: {item}")

            logger.info(f"[ImpactAnalysis] Parsed/validated AI response. Found {len(validated_analysis)} changes.")
            return DiffImpactAnalysisResponse(
                perspective_filename=perspective_filename,
                analysis=validated_analysis
            )
        except json.JSONDecodeError as json_e:
             logger.error(f"[ImpactAnalysis] Failed decode JSON: {json_e}. Raw: {ai_response_json_str}", exc_info=True)
             # Be more specific about the error
             error_detail = f"AI response for impact analysis was not valid JSON. Error near character {json_e.pos}."
             # Optionally include the problematic JSON string if safe
             # error_detail += f" Response: {ai_response_json_str[:500]}..." 
             raise HTTPException(status_code=502, detail=error_detail)
        except ValueError as val_err: # Catch specific validation errors
             logger.error(f"[ImpactAnalysis] Failed structure validation: {val_err}. Cleaned: {ai_response_json_str}", exc_info=True)
             raise HTTPException(status_code=502, detail=f"AI impact analysis response structure invalid: {val_err}")
        except Exception as e: # Catch other validation/parsing errors
             logger.error(f"[ImpactAnalysis] Failed parse/validate AI response: {e}. Cleaned: {ai_response_json_str}", exc_info=True)
             raise HTTPException(status_code=502, detail=f"AI impact analysis response failed validation: {e}")

    except HTTPException as http_exc:
        raise http_exc # Re-raise client/server errors
    except Exception as e:
        logger.error(f"[ImpactAnalysis] Unexpected error processing files: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during impact analysis: {e}")
    finally:
        # 9. Clean up temporary files using the list
        for temp_path in temp_paths_to_clean:
            if temp_path and temp_path.exists():
                try: 
                    temp_path.unlink()
                    logger.info(f"[ImpactAnalysis] Deleted temp file: {temp_path}")
                except Exception as del_e: 
                    logger.error(f"[ImpactAnalysis] Failed delete {temp_path}: {del_e}")
        # Ensure file handles are closed even if errors occurred before await
        if file1 and not file1.file._file.closed: # Check if underlying file is closed
            await file1.close()
        if file2 and not file2.file._file.closed:
            await file2.close()
            
# --- <<< END: Diff Impact Analysis Endpoint >>> ---

# --- Main execution ---
if __name__ == "__main__":
    # Determine host based on environment (e.g., Docker container vs local)
    # Docker typically uses 0.0.0.0 to be accessible outside the container
    host = os.getenv("HOST", "127.0.0.1") 
    port = int(os.getenv("PORT", 8000))
    
    # Log the UPLOAD_DIR and TEMPLATE_DIR at startup
    logger.info(f"FastAPI application starting up...")
    logger.info(f"Upload directory set to: {UPLOAD_DIR.resolve()}")
    if jinja_env:
        logger.info(f"Template directory set to: {TEMPLATE_DIR.resolve()}")
    else:
        logger.warning(f"Template directory not found or not configured.")
        
    uvicorn.run(app, host=host, port=port) # Corrected: pass app object 